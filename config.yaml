tokenizer:
    pad_id: 0
    unk_id: 1
    bos_id: 2
    eos_id: 3
    pad_token: "[PAD]"
    unk_token: "[UNK]"
    bos_token: "[BOS]"
    eos_token: "[EOS]"
    vocab_size: 30000
    min_tok_len: 256
    max_tok_len: 512
    tokenizer_path: "data/tokenizer.json"


model:
    emb_dim: 256
    hidden_dim: 256
    pff_dim: 512
    n_layers: 4
    n_heads: 8
    dropout_ratio: 0.1
    max_len: 512


train:
    n_epochs: 10
    batch_size: 32
    lr: 0.0005
    early_stop: 1
    patience: 3
    clip: 1
    iters_to_accumulate: 4